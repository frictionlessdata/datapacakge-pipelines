{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datapackage Pipelines Tutorial\n",
    "\n",
    "This tutorial is built as a Jupyter notebook which allows you to run and modify the code inline and can be used as a starting point for new Datapackage Pipelines projects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation\n",
    "\n",
    "Follow the [DataFlows Tutorial](https://github.com/datahq/dataflows/blob/master/TUTORIAL.ipynb) installation instructions.\n",
    "\n",
    "Save this tutorial in curreny working directory (right-click and save on following link): https://raw.githubusercontent.com/frictionlessdata/datapackage-pipelines/master/TUTORIAL.ipynb\n",
    "\n",
    "Start Jupyter Lab in the dataflows environment and open the datapackage pipelines tutorial notebook you downloaded."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install datapackage-pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "python3 -m pip install -qU datapackage-pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify you have the latest datapackage-pipelines version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installed version: 2.0.0\n",
      "Latest version: 2.0.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "dpp version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a flow\n",
    "\n",
    "Datapackage-pipelines uses the [DataFlows]() library's Flow objects as the basic building blocks for larger pipeline systems.\n",
    "\n",
    "It's recommended to follow the [DataFlows Tutorial]() to get a better understanding of the DataFlows concepts which will be used here.\n",
    "\n",
    "Run the following cell to create a file called `countries_population_flow.py` which scrapes a list of countries populations from wikipedia.\n",
    "\n",
    "This flow is copied from the DataFlows tutorial, the processing function `country_population` is exactly the same, the flow and how we run it is changed to integrate with Datapackage Pipelines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting countries_population_flow.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile countries_population_flow.py\n",
    "\n",
    "# notice that we don't import any datapackage-pipelines modules\n",
    "# all the flow code is written purely with the DataFlows library\n",
    "from dataflows import Flow, dump_to_path, load, add_metadata, printer, update_resource\n",
    "from xml.etree import ElementTree\n",
    "from urllib.request import urlopen\n",
    "\n",
    "\n",
    "# Generator flow step, copied from the DataFlows tutorial\n",
    "# it just spews rows of data - in this case, countries populations scraped from Wikipedia\n",
    "def country_population():\n",
    "    # Read the Wikipedia page and parse it using etree\n",
    "    page = urlopen('https://en.wikipedia.org/wiki/List_of_countries_and_dependencies_by_population').read()\n",
    "    tree = ElementTree.fromstring(page)\n",
    "    # Iterate on all tables, rows and cells\n",
    "    for table in tree.findall('.//table'):\n",
    "        if 'wikitable' in table.attrib.get('class', ''):\n",
    "            for row in table.find('tbody').findall('tr'):\n",
    "                cells = row.findall('td')\n",
    "                if len(cells) > 3:\n",
    "                    # If a matching row is found...\n",
    "                    name = cells[1].find('.//a').attrib.get('title')\n",
    "                    population = cells[2].text\n",
    "                    # ... yield a row with the information\n",
    "                    yield dict(\n",
    "                        name=name,\n",
    "                        population=population\n",
    "                    )\n",
    "\n",
    "\n",
    "# The main entrypoint for Datapackage Pipelines, each flow file should have a single flow function\n",
    "def flow(*args):\n",
    "    return Flow(\n",
    "        country_population(),\n",
    "        update_resource('res_1', **{\n",
    "            # Set a proper name for the resource\n",
    "            'name': 'countries_population',\n",
    "            # Always set a path as well, even if you don't intend to save it to the filesystem\n",
    "            'path': 'countries_population.csv',\n",
    "            # dpp:streaming property is required to let Datapackage Pipelines know it should handle this resource\n",
    "            'dpp:streaming': True,\n",
    "        })\n",
    "    )\n",
    "\n",
    "\n",
    "# Entrypoint for running the flow directly, without Datapackage Pipelines\n",
    "if __name__ == '__main__':\n",
    "    # Add a printer step and run the flow\n",
    "    Flow(flow(), printer(num_rows=1, tablefmt='html')).process()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the flow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"countries_population_flow.py\", line 49, in <module>\n",
      "    Flow(flow(), printer(num_rows=1, tablefmt='html')).process()\n",
      "TypeError: printer() got an unexpected keyword argument 'num_rows'\n"
     ]
    },
    {
     "ename": "CalledProcessError",
     "evalue": "Command 'b'python3 countries_population_flow.py\\n'' returned non-zero exit status 1.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCalledProcessError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-125-21174cb4e1e4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'sh'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'python3 countries_population_flow.py\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/envs/dataflows/lib/python3.6/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2319\u001b[0m             \u001b[0mmagic_arg_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2320\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2321\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2322\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/dataflows/lib/python3.6/site-packages/IPython/core/magics/script.py\u001b[0m in \u001b[0;36mnamed_script_magic\u001b[0;34m(line, cell)\u001b[0m\n\u001b[1;32m    140\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m                 \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscript\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshebang\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[0;31m# write a basic docstring:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<decorator-gen-109>\u001b[0m in \u001b[0;36mshebang\u001b[0;34m(self, line, cell)\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/dataflows/lib/python3.6/site-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/dataflows/lib/python3.6/site-packages/IPython/core/magics/script.py\u001b[0m in \u001b[0;36mshebang\u001b[0;34m(self, line, cell)\u001b[0m\n\u001b[1;32m    243\u001b[0m             \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_error\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m\u001b[0;34m!=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 245\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mCalledProcessError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstderr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_script\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mto_close\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mCalledProcessError\u001b[0m: Command 'b'python3 countries_population_flow.py\\n'' returned non-zero exit status 1."
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "python3 countries_population_flow.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is standard DataFlows library usage, now let's see what datapackage-pipelines provides"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a pipeline spec\n",
    "\n",
    "Datapackage-pipelines uses yaml files to to define pipelines of flow steps.\n",
    "\n",
    "Create a spec to run the countries population flow and save to a path:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing pipeline-spec.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile pipeline-spec.yaml\n",
    "countries-population:\n",
    "  pipeline:\n",
    "  - flow: countries_population_flow\n",
    "  - run: dump.to_path\n",
    "    parameters:\n",
    "      out-path: data/countries_population"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using dpp\n",
    "\n",
    "`dpp` is the CLI interface to the datapackage-pipelines library. It is used to list and run available pipelines.\n",
    "\n",
    "Let's list the available pipelines to see if our countries-population pipeline is available:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available Pipelines:\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "dpp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1A\n",
      "\u001b[2K./countries-population: \u001b[31mWAITING FOR OUTPUT\u001b[0m\n",
      "\u001b[2A\n",
      "\u001b[2K./countries-population: \u001b[33mRUNNING, processed 100 rows\u001b[0m\n",
      "\u001b[2A\n",
      "\u001b[2K./countries-population: \u001b[33mRUNNING, processed 200 rows\u001b[0m\n",
      "\u001b[2A\n",
      "\u001b[2K./countries-population: \u001b[33mRUNNING, processed 240 rows\u001b[0m\n",
      "\u001b[2A\n",
      "\u001b[2K./countries-population: \u001b[32mSUCCESS, processed 240 rows\u001b[0m\n",
      "INFO    :RESULTS:\n",
      "INFO    :SUCCESS: ./countries-population {'bytes': 6425, 'count_of_rows': 240, 'dataset_name': '_', 'hash': 'd955f25b9c927144a79fa7db92c79d79'}\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "dpp run ./countries-population"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline Dependencies\n",
    "\n",
    "Let's add another pipeline which depends on the countries-population pipeline.\n",
    "\n",
    "This time we will use just the pipeline spec yaml to write the pipeline, without any DataFlows code (although DataFlows library is used to implement the processors we are using here):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting pipeline-spec.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile pipeline-spec.yaml\n",
    "\n",
    "countries-population:\n",
    "  pipeline:\n",
    "  - flow: countries_population_flow\n",
    "  - run: dump.to_path\n",
    "    parameters:\n",
    "      out-path: data/countries_population\n",
    "\n",
    "sorted_countries_by_name:\n",
    "  dependencies:\n",
    "  - pipeline: ./countries-population\n",
    "  - datapackage: data/countries_population/datapackage.json\n",
    "  pipeline:\n",
    "  - run: load\n",
    "    parameters:\n",
    "      from: data/countries_population/datapackage.json\n",
    "      resources: ['countries_population']\n",
    "  - run: sort\n",
    "    parameters:\n",
    "      resources: ['countries_population']\n",
    "      sort-by: '{name}'\n",
    "  - run: dump.to_path\n",
    "    parameters:\n",
    "      out-path: data/sorted_countries_by_name\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clear the pipelines state using `dpp init` and list the available pipelines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available Pipelines:\n",
      "- ./double-winners (*)\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "dpp init\n",
    "dpp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that the new pipeline can't run until it's dependency is executed.\n",
    "\n",
    "You can run all the dirty dependencies using `dpp run dirty`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1A\n",
      "\u001b[2K./countries-population: \u001b[31mWAITING FOR OUTPUT\u001b[0m\n",
      "\u001b[2A\n",
      "\u001b[2K./countries-population: \u001b[33mRUNNING, processed 100 rows\u001b[0m\n",
      "\u001b[2A\n",
      "\u001b[2K./countries-population: \u001b[33mRUNNING, processed 200 rows\u001b[0m\n",
      "\u001b[2A\n",
      "\u001b[2K./countries-population: \u001b[33mRUNNING, processed 240 rows\u001b[0m\n",
      "\u001b[2A\n",
      "\u001b[2K./countries-population: \u001b[32mSUCCESS, processed 240 rows\u001b[0m\n",
      "\u001b[2A\n",
      "\u001b[2K./countries-population: \u001b[32mSUCCESS, processed 240 rows\u001b[0m\n",
      "\u001b[2K./sorted_countries_by_name: \u001b[31mWAITING FOR OUTPUT\u001b[0m\n",
      "\u001b[3A\n",
      "\u001b[2K./countries-population: \u001b[32mSUCCESS, processed 240 rows\u001b[0m\n",
      "\u001b[2K./sorted_countries_by_name: \u001b[33mRUNNING, processed 100 rows\u001b[0m\n",
      "\u001b[3A\n",
      "\u001b[2K./countries-population: \u001b[32mSUCCESS, processed 240 rows\u001b[0m\n",
      "\u001b[2K./sorted_countries_by_name: \u001b[33mRUNNING, processed 200 rows\u001b[0m\n",
      "\u001b[3A\n",
      "\u001b[2K./countries-population: \u001b[32mSUCCESS, processed 240 rows\u001b[0m\n",
      "\u001b[2K./sorted_countries_by_name: \u001b[33mRUNNING, processed 240 rows\u001b[0m\n",
      "\u001b[3A\n",
      "\u001b[2K./countries-population: \u001b[32mSUCCESS, processed 240 rows\u001b[0m\n",
      "\u001b[2K./sorted_countries_by_name: \u001b[32mSUCCESS, processed 240 rows\u001b[0m\n",
      "INFO    :RESULTS:\n",
      "INFO    :SUCCESS: ./countries-population {'bytes': 6425, 'count_of_rows': 240, 'dataset_name': '_', 'hash': 'd955f25b9c927144a79fa7db92c79d79'}\n",
      "INFO    :SUCCESS: ./sorted_countries_by_name {'bytes': 6492, 'count_of_rows': 240, 'dataset_name': '_', 'hash': 'a63e74300bbe619d4a8efba26bc43688'}\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "dpp run --dirty all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect the created datapackage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h3>countries_population</h3>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead>\n",
       "<tr><th>#  </th><th>name\n",
       "(string)            </th><th>population\n",
       "(string)           </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>1  </td><td>Abkhazia   </td><td>240,705   </td></tr>\n",
       "<tr><td>2  </td><td>Afghanistan</td><td>31,575,018</td></tr>\n",
       "<tr><td>...</td><td>           </td><td>          </td></tr>\n",
       "<tr><td>240</td><td>Zimbabwe   </td><td>14,848,905</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(<datapackage.package.Package at 0x7fc50be93128>, {})"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dataflows import Flow, load, printer\n",
    "\n",
    "Flow(\n",
    "    load('data/sorted_countries_by_name/datapackage.json'),\n",
    "    printer(num_rows=1, tablefmt='html')\n",
    ").process()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline processors\n",
    "\n",
    "Datapackage Pipelines has a standard library of processors, like the `sort` processor used previously. These processors correspond to DataFlows standard library processors.\n",
    "\n",
    "See the [Datapackage Pipelines README](https://github.com/frictionlessdata/datapackage-pipelines/blob/master/README.md) for reference and usage examples.\n",
    "\n",
    "An example showing usage of common processors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting pipeline-spec.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile pipeline-spec.yaml\n",
    "\n",
    "double-winners:\n",
    "  pipeline:\n",
    "  - run: load\n",
    "    parameters:\n",
    "      name: emmies\n",
    "      from: https://raw.githubusercontent.com/datahq/dataflows/master/data/emmy.csv\n",
    "  - run: load\n",
    "    parameters:\n",
    "      name: oscars\n",
    "      from: https://raw.githubusercontent.com/datahq/dataflows/master/data/academy.csv\n",
    "      # encoding: utf8\n",
    "    cache: true\n",
    "  - run: filter\n",
    "    parameters:\n",
    "      resources: ['emmies']\n",
    "      in:\n",
    "      - winner: 1\n",
    "  - run: concatenate\n",
    "    parameters:\n",
    "      target: {'name': 'emmies_filtered'}\n",
    "      resources: ['emmies']\n",
    "      fields:\n",
    "        emmy_nominee: ['nominee']\n",
    "  - run: join\n",
    "    parameters:\n",
    "      source:\n",
    "        name: 'emmies_filtered'\n",
    "        key: ['emmy_nominee']\n",
    "        delete: true\n",
    "      target:\n",
    "        name: 'oscars'\n",
    "        key: ['Name']\n",
    "      fields: {}\n",
    "      full: false\n",
    "  - run: filter\n",
    "    parameters:\n",
    "      in:\n",
    "      - Winner: \"1\"\n",
    "  - run: dump.to_path\n",
    "    parameters:\n",
    "      out-path: data/double_winners\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1A\n",
      "\u001b[2K./double-winners: \u001b[31mWAITING FOR OUTPUT\u001b[0m\n",
      "\u001b[2A\n",
      "\u001b[2K./double-winners: \u001b[33mRUNNING, processed 98 rows\u001b[0m\n",
      "\u001b[2A\n",
      "\u001b[2K./double-winners: \u001b[32mSUCCESS, processed 98 rows\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO    :RESULTS:\n",
      "INFO    :SUCCESS: ./double-winners {'bytes': 6766, 'count_of_rows': 98, 'dataset_name': '_', 'hash': 'bc61b69dc87b0da0348049802c616d95'}\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "dpp run ./double-winners"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the datapackage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h3>oscars</h3>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead>\n",
       "<tr><th>#  </th><th>Year\n",
       "(string)          </th><th style=\"text-align: right;\">   Ceremony\n",
       "(integer)</th><th>Award\n",
       "(string)               </th><th style=\"text-align: right;\">  Winner\n",
       "(string)</th><th>Name\n",
       "(string)                  </th><th>Film\n",
       "(string)                           </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>1  </td><td>1931/1932</td><td style=\"text-align: right;\"> 5</td><td>Actress       </td><td style=\"text-align: right;\">1</td><td>Helen Hayes      </td><td>The Sin of Madelon Claudet</td></tr>\n",
       "<tr><td>2  </td><td>1932/1933</td><td style=\"text-align: right;\"> 6</td><td>Actress       </td><td style=\"text-align: right;\">1</td><td>Katharine Hepburn</td><td>Morning Glory             </td></tr>\n",
       "<tr><td>...</td><td>         </td><td style=\"text-align: right;\">  </td><td>              </td><td style=\"text-align: right;\"> </td><td>                 </td><td>                          </td></tr>\n",
       "<tr><td>98 </td><td>2015     </td><td style=\"text-align: right;\">88</td><td>Honorary Award</td><td style=\"text-align: right;\">1</td><td>Gena Rowlands    </td><td>                          </td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(<datapackage.package.Package at 0x7fc50bb55f98>, {})"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dataflows import Flow, printer, load\n",
    "Flow(load('data/double_winners/datapackage.json'), printer(tablefmt='html', num_rows=1)).process()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipelines Server\n",
    "\n",
    "Running pipelines on your laptop is fine for many use-cases but sometimes you want to run pipelines in a more reproducible, scalable and automatic fashion.\n",
    "\n",
    "The Datapackage Pipelines Server is a Docker image which provides the core functionality to achieve this.\n",
    "\n",
    "To start a local pipelines server for development, you will need to install Docker for [Windows](https://store.docker.com/editions/community/docker-ce-desktop-windows),\n",
    "[Mac](https://store.docker.com/editions/community/docker-ce-desktop-mac) or [Linux](https://docs.docker.com/install/)\n",
    "\n",
    "Pull the datapackage-pipelines image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using default tag: latest\n",
      "latest: Pulling from frictionlessdata/datapackage-pipelines\n",
      "Digest: sha256:50fd5b40523146af0e46275f836357bf27097c1d9c83726b03da884e56d385bb\n",
      "Status: Image is up to date for frictionlessdata/datapackage-pipelines:latest\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "docker pull frictionlessdata/datapackage-pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start a local pipelines server, mounting the current working directory into the container:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99e4537cceb6ac07674b9023a2c8d668730a84f945b68a810e9d23f5798033fd\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "docker run -d --name dpp -v `pwd`:/pipelines:rw -p 5000:5000 frictionlessdata/datapackage-pipelines server"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After a few seconds, the pipelines dashboad should be available at http://localhost:5000\n",
    "\n",
    "New / modified pipelines and dirty dependencies are executed by the pipelines server automatically.\n",
    "\n",
    "The server also supports scheduled pipelines for periodical execution.\n",
    "\n",
    "Let's see this in action:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting beatles.csv\n"
     ]
    }
   ],
   "source": [
    "%%writefile pipeline-spec.yaml\n",
    "\n",
    "countries-population:\n",
    "  schedule:\n",
    "    # minute hour day_of_week day_of_month month_of_year\n",
    "    crontab: '* * * * *'\n",
    "  pipeline:\n",
    "  - flow: countries_population_flow\n",
    "  - run: dump.to_path\n",
    "    parameters:\n",
    "      out-path: data/countries_population\n",
    "\n",
    "sorted_countries_by_name:\n",
    "  dependencies:\n",
    "  - pipeline: ./countries-population\n",
    "  - datapackage: data/countries_population/datapackage.json\n",
    "  pipeline:\n",
    "  - run: load\n",
    "    parameters:\n",
    "      from: data/countries_population/datapackage.json\n",
    "      resources: ['countries_population']\n",
    "  - run: sort\n",
    "    parameters:\n",
    "      resources: ['countries_population']\n",
    "      sort-by: '{name}'\n",
    "  - run: dump.to_path\n",
    "    parameters:\n",
    "      out-path: data/sorted_countries_by_name\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect the Pipelines server logs and wait for `Update Pipelines` task to complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-10-16 09:03:00,053: INFO/ForkPoolWorker-1(86)] Task datapackage_pipelines.celery_tasks.celery_tasks.update_pipelines[f89eb25b-0143-4cb4-8e5b-56d7d4abec0c] succeeded in 0.013886921020457521s: None\n",
      "[2018-10-16 09:04:00,029: INFO/MainProcess(36)] Scheduler: Sending due task /management (datapackage_pipelines.celery_tasks.celery_tasks.update_pipelines)\n",
      "[2018-10-16 09:04:00,036: INFO/MainProcess(37)] Received task: datapackage_pipelines.celery_tasks.celery_tasks.update_pipelines[3e6f3b44-fc56-4a9a-b08a-5fc03f06747d]  \n",
      "[2018-10-16 09:04:00,038: INFO/ForkPoolWorker-1(86)] Update Pipelines (update)\n",
      "[2018-10-16 09:04:00,065: INFO/ForkPoolWorker-1(86)] Task datapackage_pipelines.celery_tasks.celery_tasks.update_pipelines[3e6f3b44-fc56-4a9a-b08a-5fc03f06747d] succeeded in 0.027734030009014532s: None\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "docker logs dpp --tail 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Refresh the dashboard to see the new pipelines and execution logs: http://localhost:5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
